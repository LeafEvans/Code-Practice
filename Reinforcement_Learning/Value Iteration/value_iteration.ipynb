{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 值迭代算法\n",
    "作者：stzhao\n",
    "github: https://github.com/zhaoshitian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、定义环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“joyrl (Python 3.7.16)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
      "\u001b[1;31m命令: \"conda install -n joyrl ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": [
    "# 导入numpy库，用于数值计算和数组操作\n",
    "import numpy as np \n",
    "\n",
    "# 导入自定义环境DrunkenWalkEnv\n",
    "# 这是一个简单的网格世界环境,模拟醉汉行走问题\n",
    "# 醉汉在网格中移动,目标是到达终点位置\n",
    "from envs.simple_grid import DrunkenWalkEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed=1):\n",
    "    \"\"\"设置所有随机数种子,确保实验的可重复性\n",
    "    \n",
    "    Args:\n",
    "        env: 强化学习环境\n",
    "        seed: 随机数种子,默认为1\n",
    "        \n",
    "    功能:\n",
    "        - 设置环境的随机数种子\n",
    "        - 设置numpy的随机数种子 \n",
    "        - 设置Python random模块的随机数种子\n",
    "        - 设置Python哈希种子\n",
    "    \"\"\"\n",
    "    import numpy as np \n",
    "    import random \n",
    "    import os \n",
    "    # 设置环境的随机数种子\n",
    "    env.seed(seed)\n",
    "    # 设置numpy的随机数种子\n",
    "    np.random.seed(seed)\n",
    "    # 设置Python random模块的随机数种子 \n",
    "    random.seed(seed)\n",
    "    # 设置Python哈希种子\n",
    "    os.environ['PATHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Code_Practice\\Reinforcement_Learning\\Value Iteration\\.conda\\Lib\\site-packages\\gym\\utils\\seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# 创建醉汉行走环境实例\n",
    "# map_name='theAlley'表示使用小巷地图\n",
    "# theAlley地图是一个简单的网格世界,醉汉需要在其中导航到达目标\n",
    "env = DrunkenWalkEnv(map_name='theAlley')\n",
    "\n",
    "# 设置随机数种子以确保实验可重复性\n",
    "# seed=1是一个任意选择的种子值\n",
    "# 这将确保每次运行代码时得到相同的随机数序列\n",
    "all_seed(env, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、价值迭代算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.005, discount_factor=0.9):\n",
    "    \"\"\"\n",
    "    实现价值迭代算法\n",
    "\n",
    "    参数:\n",
    "    env: 强化学习环境\n",
    "    theta: 收敛阈值，默认为0.005\n",
    "    discount_factor: 折扣因子，默认为0.9\n",
    "\n",
    "    返回:\n",
    "    Q: 最优动作价值函数\n",
    "    \"\"\"\n",
    "    # 初始化Q表格，大小为[状态数, 动作数]\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    count = 0  # 迭代次数计数器\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0  # 用于记录Q值的最大变化\n",
    "        Q_tmp = np.zeros([env.nS, env.nA])  # 临时Q表格，用于存储更新后的Q值\n",
    "\n",
    "        # 遍历所有状态和动作\n",
    "        for state in range(env.nS):\n",
    "            for action in range(env.nA):\n",
    "                expected_reward = 0.0  # 期望奖励\n",
    "                expected_value = 0.0  # 期望价值\n",
    "\n",
    "                # 遍历当前状态-动作对的所有可能结果\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    expected_reward += prob * reward  # 计算期望奖励\n",
    "                    expected_value += prob * max(Q[next_state, :])  # 计算期望价值\n",
    "\n",
    "                # 更新Q值\n",
    "                Q_tmp[state, action] = discount_factor * expected_value + expected_reward\n",
    "\n",
    "                # 更新delta，记录最大Q值变化\n",
    "                delta = max(delta, abs(Q_tmp[state, action] - Q[state, action]))\n",
    "\n",
    "        Q = Q_tmp  # 更新Q表格\n",
    "        count += 1  # 迭代次数加1\n",
    "\n",
    "        # 检查是否收敛或达到最大迭代次数\n",
    "        if delta < theta or count > 100:\n",
    "            break\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.25015697e+22 2.53142659e+22 4.50031394e+22 2.53142659e+22]\n",
      " [2.81269621e+22 5.41444021e+22 1.01257064e+23 5.41444021e+22]\n",
      " [6.32856648e+22 1.21824905e+23 2.27828393e+23 1.21824905e+23]\n",
      " [1.42392746e+23 2.74106036e+23 5.12613885e+23 2.74106036e+23]\n",
      " [3.20383678e+23 5.76690620e+23 1.15338124e+24 5.76690620e+23]\n",
      " [7.20863276e+23 1.38766181e+24 2.59510779e+24 1.38766181e+24]\n",
      " [1.62194237e+24 3.12223906e+24 5.83899253e+24 3.12223906e+24]\n",
      " [3.64937033e+24 7.02503789e+24 1.31377332e+25 7.02503789e+24]\n",
      " [8.21108325e+24 1.47799498e+25 2.95598997e+25 1.47799498e+25]\n",
      " [1.84749373e+25 3.55642543e+25 6.65097743e+25 3.55642543e+25]\n",
      " [4.15686089e+25 8.00195722e+25 1.49646992e+26 8.00195722e+25]\n",
      " [9.35293701e+25 1.80044037e+26 3.36705732e+26 1.80044037e+26]\n",
      " [5.89235032e+26 7.36543790e+26 7.57587898e+26 7.36543790e+26]]\n"
     ]
    }
   ],
   "source": [
    "# 使用价值迭代算法计算最优动作价值函数Q\n",
    "Q = value_iteration(env)\n",
    "\n",
    "# 打印最终的Q表格\n",
    "# Q表格的形状为[状态数, 动作数]\n",
    "# 每一行代表一个状态\n",
    "# 每一列代表在该状态下采取相应动作的价值\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_22924\\2001149784.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  policy = [int(np.argwhere(policy[i] == 1)[0]) for i in range(env.nS)]\n"
     ]
    }
   ],
   "source": [
    "# 初始化策略矩阵,大小为[状态数, 动作数]\n",
    "# 每一行代表一个状态,每一列代表该状态下采取相应动作的概率\n",
    "policy = np.zeros([env.nS, env.nA])\n",
    "\n",
    "# 遍历所有状态\n",
    "for state in range(env.nS):\n",
    "    # 找出当前状态下价值最大的动作\n",
    "    best_action = np.argmax(Q[state, :])\n",
    "    # 将最优动作的概率设为1,其他动作概率为0\n",
    "    policy[state, best_action] = 1\n",
    "\n",
    "# 将策略矩阵转换为一维数组\n",
    "# 对每个状态,找出概率为1的动作的索引\n",
    "# np.argwhere返回非零元素的索引,取第一个索引[0]并转为整数\n",
    "policy = [int(np.argwhere(policy[i] == 1)[0]) for i in range(env.nS)]\n",
    "# 打印最终的确定性策略,每个数字代表在对应状态下应该采取的动作\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 1000 \n",
    "\n",
    "def test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试的成功率是： 0.638\n"
     ]
    }
   ],
   "source": [
    "test(env, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
